{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HOMEWORK 7\n",
    "### EE 578B  - Winter 2021\n",
    "### Due Date: Wednesday, Mar 3rd, 2021 @ 11:59 PM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the Markov Decision Process with the following graph and action structure.  (SEE PDF)\n",
    "\n",
    "## 1. Transition Kernel Constraints\n",
    "#### (PTS:0-2)\n",
    "Write down the incidence matrices for the graph.  \n",
    "\\begin{align*}\n",
    "E_i \\in \\mathbb{R}^{|\\mathcal{S}| \\times |\\mathcal{E}|}, \\quad\n",
    "E_o \\in \\mathbb{R}^{|\\mathcal{S}| \\times |\\mathcal{E}|}, \\quad\n",
    "P \\in \\mathbb{R}^{|\\mathcal{S}| \\times |\\mathcal{A}|}, \\quad\n",
    "A \\in \\mathbb{R}^{|\\mathcal{S}| \\times |\\mathcal{A}|}, \\quad\n",
    "W \\in \\mathbb{R}^{|\\mathcal{E}| \\times |\\mathcal{A}|}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The incidence matrics can be written as follows\n",
    "\n",
    "$$E_i = \\begin{bmatrix}0 & 1 & 0 & 0 & 1 & 0 \\\\ 0 & 0 & 0 & 1 & 0 & 0 \\\\ 1 & 0 & 0 & 0 & 0 & 1 \\\\ 0 & 0 & 1 & 0 & 0 & 0 \\end{bmatrix}$$\n",
    "\n",
    "$$E_o = \\begin{bmatrix}1 & 0 & 0 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0 & 0 & 0 \\\\ 0 & 0 & 1 & 1 & 0 & 0 \\\\ 0 & 0 & 0 & 0 & 1 & 1 \\end{bmatrix}$$\n",
    "\n",
    "$$P = \\begin{bmatrix}0 & 1 & 0 & 0 & 1 & 0.5 \\\\ 0 & 0 & 0 & 0.5 & 0 & 0 \\\\ 1 & 0 & 0 & 0 & 0 & 0.5 \\\\ 0 & 0 & 1 & 0.5 & 0 & 0 \\end{bmatrix}$$\n",
    "\n",
    "$$A = \\begin{bmatrix}1 & 0 & 0 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0 & 0 & 0 \\\\ 0 & 0 & 1 & 1 & 0 & 0 \\\\ 0 & 0 & 0 & 0 & 1 & 1 \\end{bmatrix}$$\n",
    "\n",
    "$$W = \\begin{bmatrix}1 & 0 & 0 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0 & 0 & 0 \\\\ 0 & 0 & 1 & 0.5 & 0 & 0 \\\\ 0 & 0 & 0 & 0.5 & 0 & 1 \\\\ 0 & 0 & 0 & 0 & 1 & 0.5 \\\\ 0 & 0 & 0 & 0 & 0 & 0.5\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "     \n",
    "#### (PTS:0-2)\n",
    "For the incidence matrices given above show the following identities\n",
    "\\begin{align*}\n",
    "\\mathbf{1}^TE_i & = \\mathbf{1}^TE_o = \\mathbf{1}^T \\\\\n",
    "\\mathbf{1}^TA & = \\mathbf{1}^TP = \n",
    "\\\\\n",
    "\\mathbf{1}^TW & = \\mathbf{1}^T \\\\\n",
    "E_iW & = P, \\quad E_oW=A\n",
    "\\end{align*}\n",
    "where the dimension of each $\\mathbf{1}$ is determined by context.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "     \n",
    "#### (PTS:0-2)\n",
    "Consider two policies with the following actions chosen from each state\n",
    "\\begin{align*}\n",
    "\\textbf{Policy 1:} \\qquad \\quad & \\text{State 1: Action 1}, \\quad \\text{State 2: Action 2}, \\\\\n",
    "& \\text{State 3: Action 4}, \\quad \\text{State 4: Action 6} \\\\\n",
    "\\textbf{Policy 2:} \\qquad \\quad &\\text{State 1: Action 1}, \\quad \\text{State 2: Action 2}, \\\\\n",
    "& \\text{State 3: }\n",
    "\\begin{matrix}\n",
    "\\text{50\\% Action 3} \\\\\n",
    "\\text{50\\% Action 4}\n",
    "\\end{matrix}, \\quad\n",
    "\\text{State 4: }\n",
    "\\begin{matrix}\n",
    "\\text{50\\% Action 5} \\\\\n",
    "\\text{50\\% Action 6}\n",
    "\\end{matrix}\n",
    "\\end{align*}\n",
    "Write each policy in matrix form $\\Pi \\in \\mathbb{R}^{6 \\times 4}$.  Compute the corresponding Markov matrix $M = P\\Pi$.  Also show that $A\\Pi = I$ for each policy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (PTS:0-4)\n",
    "The stationary (state) distribution associated with each Markov chain is the solution to the equation $\\rho = M \\rho$. Compute this stationary distribution by finding the eigenvector with eigenvalue 1. (You can use the function eig in Matlab or numpy.linalg.eig in Python.). Make sure to scale the eigenvector so that it is an appropriate probability distribution that sums to 1 and has all positive values.  Compute the corresponding action distribution $y$ as $y = \\Pi \\rho$.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (PTS:0-2)\n",
    "Show that each $y$ from the previous part satisfies $Py = Ay$ and $\\mathbf{1}^Ty = 1$. Compute the corresponding edge mass vector for each $x = Wy$.  Show that $x$ is in the nullspace of $E = E_i - E_o$.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Infinite Horizon, Average Reward\n",
    "    \n",
    "Consider the following optimization problem for finding the optimal steady-state action  distribution $y  \\in \\mathbb{R}^{|\\mathcal{A}|}$\n",
    "\\begin{align}\n",
    "\\max_{y} & \\quad r^Ty \\\\\n",
    "\\text{s.t.} & \\quad Py = Ay, \\ \\mathbf{1}^Ty = 1, \\ y \\geq 0 \\notag\n",
    "\\end{align}\n",
    "for reward vector $r \\in \\mathbb{R}^{|\\mathcal{A}|}$.  \n",
    "    \n",
    "#### (PTS:0-2)\n",
    "Write the dual optimization problem with dual variables $\\lambda \\in \\mathbb{R}$ associated with the constraint $\\mathbf{1}^Ty=1$, $v \\in \\mathbb{R}^{|\\mathcal{S}|}$ associated with constraint $Py=Ay$, $\\mu \\in \\mathbb{R}^{|\\mathcal{A}|}_+$ associated with the constraint $y \\geq 0$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        \n",
    "#### (PTS:0-2)\n",
    "The KKT conditions at optimum (for either the primal or dual problem) are given by\n",
    "\\begin{align*}\n",
    "r^T - \\lambda \\mathbf{1}^T + v^T(P-A) + \\mu^T & = 0, \\quad \\mu \\geq 0 \\\\\n",
    "Py-Ay = 0, \\quad \\mathbf{1}^Ty & = 1, \\quad y \\geq 0  \\\\\n",
    "\\mu^Ty & = 0 \n",
    "\\end{align*}\n",
    "        \n",
    "Use these conditions to show that $\\lambda$ is an upper bound on the primal objective $r^Ty$ for any feasible $y$.  What does $\\mu^Ty$ represent for a specific $y$?  What does the condition $\\mu^Ty = 0$ imply about the optimal $y$?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        \n",
    "#### (PTS:0-4)\n",
    "Use cvx or cvxpy to solve the above optimization problem for the transition kernel given initially and each reward vector\n",
    "\\begin{align*}\n",
    "r^T & =\n",
    "\\begin{bmatrix}\n",
    "1 & 2 & 3 & 4 & 5 & 6\n",
    "\\end{bmatrix} \\\\\n",
    "r^T & =\n",
    "\\begin{bmatrix}\n",
    "1 & 1 & 1 & 1 & 1 & 1\n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "What is the optimal joint distribution $y$ in each case?  What is the expected average reward $r^Ty$ in each case? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (PTS:0-2)\n",
    "What is the steady-state state distribution associated with each solution $\\rho=Ay$?  \n",
    "What is the optimal policy associated with $y$?  Use the formula\n",
    "\\begin{align*}\n",
    "    (\\pi_s)_a = \\frac{y_a}{\\rho_s} = \\frac{y_a}{\\sum_{a \\in \\mathcal{A}_s} y_a}\n",
    "\\end{align*}\n",
    "You could also put the policy in matrix form using the formula\n",
    "\\begin{align*}\n",
    "\\Pi = \\text{diag}(y)A^T\\text{diag}(\\rho)^{-1}\n",
    "\\end{align*}\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   \n",
    "#### (PTS:0-2) Now suppose you apply the policy \n",
    "\\begin{align*}\n",
    "\\Pi = \\begin{bmatrix}\n",
    "1 & 0 & 0 & 0 \\\\\n",
    "0 & 1 & 0 & 0 \\\\\n",
    "0 & 0 & 0.2 & 0 \\\\\n",
    "0 & 0 & 0.8 & 0 \\\\\n",
    "0 & 0 & 0 & 0.2 \\\\\n",
    "0 & 0 & 0 & 0.8 \n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "What reward do you achieve in each case? (Hint: compute $\\rho$ such that $\\rho = P\\Pi \\rho$ and then $y$ using $y=\\Pi \\rho$.)\n",
    "How much does this reward differ from the optimal average reward?  How does this difference relate to the quantity $\\mu^Ty$ where $\\mu$ is the optimal dual variable?\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    \n",
    "\n",
    "\n",
    "## 3. Finite Horizon, Total Reward\n",
    "    \n",
    "Consider the following optimization problem for finding the optimal finite horizon policy.  \n",
    "\\begin{align}\n",
    "\\max_{y(t), \\ t \\in \\mathcal{T}} & \\quad \\sum_{t=0}^{T-1} r(t)^Ty(t) + g^TAy(T) \\\\\n",
    "\\text{s.t.} & \\quad Ay(0) = \\rho(0), \\quad y(0) \\geq 0 \\notag \\\\\n",
    "& \\quad Ay(t+1) = Py(t), \\quad y(t+1) \\geq 0, \\ \\ t \\in \\mathcal{T} \\notag\n",
    "\\end{align}\n",
    "where $\\mathcal{T} = \\{0,\\dots, T-1\\}$, $\\rho(0) \\in \\mathbb{R}^{|\\mathcal{S}|}$ is a given initial state distribution, and $g \\in \\mathbb{R}^{|\\mathcal{S}|}$ is a final cost on each of the states.  \n",
    "    \n",
    "#### (PTS:0-4)\n",
    "Write the dual optimization problem with dual variables $v(0) \\in \\mathbb{R}^{|\\mathcal{S}|}$ associated with the constraint $Ay(0)=\\rho(0)$, $v(t+1) \\in \\mathbb{R}^{|\\mathcal{S}|}$ associated with constraint $Py(t)=Ay(t+1)$, and $\\mu(t) \\in \\mathbb{R}^{|\\mathcal{A}|}_+$ associated with the constraint $y(t) \\geq 0$. \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "#### (PTS:0-4)\n",
    "The KKT optimality conditions for the primal and dual optimization problems\n",
    "are given by\n",
    "\\begin{align*}\n",
    "g^TA - v(T)A + \\mu(T)^T & = 0, \\quad \\mu(T) \\geq 0  \\\\\n",
    "r(t)^T +v(t+1)^TP-v(t)^TA+\\mu(t)^T & = 0, \\quad \\mu(t) \\geq 0, \\quad t \\in \\mathcal{T} \\\\\n",
    "Ay(0) & = \\rho(0), \\quad y(0) \\geq 0  \\\\\n",
    "Ay(t+1) & = Py(t), \\quad y(t+1) \\geq 0, \\quad t \\in \\mathcal{T} \\\\\n",
    "\\mu(t)^Ty(t) & = 0, \\quad t \\in \\mathcal{T},\\ t=T\n",
    "\\end{align*}\n",
    "                \n",
    "Use these conditions to show that $v(0)^T\\rho(0)$ is an upper bound on the primal objective $\\sum_t r(t)^Ty(t) + g^TAy(T)$ for any feasible $y(t)$ that satisfies the mass flow equations.  What does $\\sum_t \\mu(t)^Ty(t)$ represent for a specific mass flow $y(t), t \\in \\mathcal{T}$.  \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (PTS:0-4)\n",
    "Use cvx or cvxpy to solve the above optimization problem for the MDP given initially with the following rewards\n",
    "\\begin{align*}\n",
    "r(t)^T & =\n",
    "\\begin{bmatrix}\n",
    "2 & 1 & 2 & 1 & 2 & 1\n",
    "\\end{bmatrix} \\text{ for } t \\in \\mathcal{T}, \\quad g^T =\n",
    "\\begin{bmatrix}\n",
    "1 & 1 & 1 & 1\n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "for ten time steps\n",
    " $T=10$\n",
    "and initial distribution $\\rho(0) = \\begin{bmatrix}0.25 & 0.25 & 0.25 & 0.25 \\end{bmatrix}^T$\n",
    "\n",
    "        \n",
    "What is the optimal action distribution $y(t)$ at each time step?  What is the expected total reward $\\sum_t r(t)^Ty(t)$? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        \n",
    "#### (PTS:0-4)\n",
    "What is the policy $\\Pi(t)$ chosen at each time step?  Use the formula\n",
    "\\begin{align*}\n",
    "    (\\pi_s)_a(t) = \\tfrac{y_a(t)}{\\rho_s(t)} = \\tfrac{y_a(t}{\\sum_{a \\in \\mathcal{A}_s} y_a(t)}\n",
    "\\end{align*}\n",
    "where $\\rho(t) = Ay(t)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (PTS:0-4) Now suppose you apply the policy \n",
    "\\begin{align*}\n",
    "\\Pi(t) = \\begin{bmatrix}\n",
    "1 & 0 & 0 & 0 \\\\\n",
    "0 & 1 & 0 & 0 \\\\\n",
    "0 & 0 & 0.2 & 0 \\\\\n",
    "0 & 0 & 0.8 & 0 \\\\\n",
    "0 & 0 & 0 & 0.2 \\\\\n",
    "0 & 0 & 0 & 0.8 \n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "at each time step.\n",
    "Start by computing $y(0) = \\Pi(0)\\rho(0)$. $\\rho(t)$ is then given by $Py(0) = \\rho(1)$. Use $\\rho(1)$ to compute $y(1)= \\Pi(1)\\rho(1)$, etc.  What total reward do you achieve? What is the quantity $\\sum_t \\mu(t)^Ty(t)$?  How does this relate the total reward to the optimal total reward?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
