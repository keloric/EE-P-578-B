{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HOMEWORK 7\n",
    "### EE 578B  - Winter 2021\n",
    "### Due Date: Wednesday, Mar 3rd, 2021 @ 11:59 PM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.lines as mlines\n",
    "import cvxpy as cp\n",
    "from mpl_toolkits.mplot3d.art3d import Poly3DCollection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the Markov Decision Process with the following graph and action structure.  (SEE PDF)\n",
    "\n",
    "## 1. Transition Kernel Constraints\n",
    "#### (PTS:0-2)\n",
    "Write down the incidence matrices for the graph.  \n",
    "\\begin{align*}\n",
    "E_i \\in \\mathbb{R}^{|\\mathcal{S}| \\times |\\mathcal{E}|}, \\quad\n",
    "E_o \\in \\mathbb{R}^{|\\mathcal{S}| \\times |\\mathcal{E}|}, \\quad\n",
    "P \\in \\mathbb{R}^{|\\mathcal{S}| \\times |\\mathcal{A}|}, \\quad\n",
    "A \\in \\mathbb{R}^{|\\mathcal{S}| \\times |\\mathcal{A}|}, \\quad\n",
    "W \\in \\mathbb{R}^{|\\mathcal{E}| \\times |\\mathcal{A}|}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The incidence matrics can be written as follows\n",
    "\n",
    "$$E_i = \\begin{bmatrix}0 & 1 & 0 & 0 & 1 & 0 \\\\ 0 & 0 & 0 & 1 & 0 & 0 \\\\ 1 & 0 & 0 & 0 & 0 & 1 \\\\ 0 & 0 & 1 & 0 & 0 & 0 \\end{bmatrix}$$\n",
    "\n",
    "$$E_o = \\begin{bmatrix}1 & 0 & 0 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0 & 0 & 0 \\\\ 0 & 0 & 1 & 1 & 0 & 0 \\\\ 0 & 0 & 0 & 0 & 1 & 1 \\end{bmatrix}$$\n",
    "\n",
    "$$P = \\begin{bmatrix}0 & 1 & 0 & 0 & 1 & 0.5 \\\\ 0 & 0 & 0 & 0.5 & 0 & 0 \\\\ 1 & 0 & 0 & 0 & 0 & 0.5 \\\\ 0 & 0 & 1 & 0.5 & 0 & 0 \\end{bmatrix}$$\n",
    "\n",
    "$$A = \\begin{bmatrix}1 & 0 & 0 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0 & 0 & 0 \\\\ 0 & 0 & 1 & 1 & 0 & 0 \\\\ 0 & 0 & 0 & 0 & 1 & 1 \\end{bmatrix}$$\n",
    "\n",
    "$$W = \\begin{bmatrix}1 & 0 & 0 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0 & 0 & 0 \\\\ 0 & 0 & 1 & 0.5 & 0 & 0 \\\\ 0 & 0 & 0 & 0.5 & 0 & 1 \\\\ 0 & 0 & 0 & 0 & 1 & 0.5 \\\\ 0 & 0 & 0 & 0 & 0 & 0.5\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "     \n",
    "#### (PTS:0-2)\n",
    "For the incidence matrices given above show the following identities\n",
    "\\begin{align*}\n",
    "\\mathbf{1}^TE_i & = \\mathbf{1}^TE_o = \\mathbf{1}^T \\\\\n",
    "\\mathbf{1}^TA & = \\mathbf{1}^TP = 1^T\n",
    "\\\\\n",
    "\\mathbf{1}^TW & = \\mathbf{1}^T \\\\\n",
    "E_iW & = P, \\quad E_oW=A\n",
    "\\end{align*}\n",
    "where the dimension of each $\\mathbf{1}$ is determined by context.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the first identity,\n",
    "\n",
    "$$1^T E_i = \\begin{bmatrix}1 & 1 & 1 & 1 \\end{bmatrix} * \\begin{bmatrix}0 & 1 & 0 & 0 & 1 & 0 \\\\ 0 & 0 & 0 & 1 & 0 & 0 \\\\ 1 & 0 & 0 & 0 & 0 & 1 \\\\ 0 & 0 & 1 & 0 & 0 & 0 \\end{bmatrix} = \\begin{bmatrix}1 & 1 & 1 & 1 & 1 & 1\\end{bmatrix}$$\n",
    "\n",
    "$$1^T E_o = \\begin{bmatrix}1 & 1 & 1 & 1 \\end{bmatrix} * \\begin{bmatrix}1 & 0 & 0 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0 & 0 & 0 \\\\ 0 & 0 & 1 & 1 & 0 & 0 \\\\ 0 & 0 & 0 & 0 & 1 & 1 \\end{bmatrix} = \\begin{bmatrix}1 & 1 & 1 & 1 & 1 & 1\\end{bmatrix}$$\n",
    "\n",
    "Thus, we see that $1^T E_i = 1^T E_o = 1^T$.\n",
    "\n",
    "For the second identity,\n",
    "\n",
    "$$1^T A = \\begin{bmatrix}1 & 1 & 1 & 1 \\end{bmatrix} * \\begin{bmatrix}1 & 0 & 0 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0 & 0 & 0 \\\\ 0 & 0 & 1 & 1 & 0 & 0 \\\\ 0 & 0 & 0 & 0 & 1 & 1 \\end{bmatrix} = \\begin{bmatrix}1 & 1 & 1 & 1 & 1 & 1\\end{bmatrix}$$\n",
    "\n",
    "$$1^T P = \\begin{bmatrix}1 & 1 & 1 & 1 \\end{bmatrix} * \\begin{bmatrix}0 & 1 & 0 & 0 & 1 & 0.5 \\\\ 0 & 0 & 0 & 0.5 & 0 & 0 \\\\ 1 & 0 & 0 & 0 & 0 & 0.5 \\\\ 0 & 0 & 1 & 0.5 & 0 & 0 \\end{bmatrix} = \\begin{bmatrix}1 & 1 & 1 & 1 & 1 & 1\\end{bmatrix}$$\n",
    "\n",
    "Thus, we can see that $1^T A = 1^T P = 1^T$.\n",
    "\n",
    "For the third identity,\n",
    "\n",
    "$$1^T W = \\begin{bmatrix}1 & 1 & 1 & 1 & 1 & 1\\end{bmatrix} * \\begin{bmatrix}1 & 0 & 0 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0 & 0 & 0 \\\\ 0 & 0 & 1 & 0.5 & 0 & 0 \\\\ 0 & 0 & 0 & 0.5 & 0 & 1 \\\\ 0 & 0 & 0 & 0 & 1 & 0.5 \\\\ 0 & 0 & 0 & 0 & 0 & 0.5\\end{bmatrix} = \\begin{bmatrix}1 & 1 & 1 & 1 & 1 & 1\\end{bmatrix}$$\n",
    "\n",
    "Thus, we can see that $1^T W = 1^T$.\n",
    "\n",
    "For our last identity,\n",
    "\n",
    "$$E_i W = \\begin{bmatrix}0 & 1 & 0 & 0 & 1 & 0 \\\\ 0 & 0 & 0 & 1 & 0 & 0 \\\\ 1 & 0 & 0 & 0 & 0 & 1 \\\\ 0 & 0 & 1 & 0 & 0 & 0 \\end{bmatrix} * \\begin{bmatrix}1 & 0 & 0 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0 & 0 & 0 \\\\ 0 & 0 & 1 & 0.5 & 0 & 0 \\\\ 0 & 0 & 0 & 0.5 & 0 & 1 \\\\ 0 & 0 & 0 & 0 & 1 & 0.5 \\\\ 0 & 0 & 0 & 0 & 0 & 0.5\\end{bmatrix} = \\begin{bmatrix}0 & 1 & 0 & 0 & 1 & 0.5 \\\\ 0 & 0 & 0 & 0.5 & 0 & 0 \\\\ 1 & 0 & 0 & 0 & 0 & 0.5 \\\\ 0 & 0 & 1 & 0.5 & 0 & 0 \\end{bmatrix} = P$$\n",
    "\n",
    "$$E_o W = \\begin{bmatrix}1 & 0 & 0 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0 & 0 & 0 \\\\ 0 & 0 & 1 & 1 & 0 & 0 \\\\ 0 & 0 & 0 & 0 & 1 & 1 \\end{bmatrix} * \\begin{bmatrix}1 & 0 & 0 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0 & 0 & 0 \\\\ 0 & 0 & 1 & 0.5 & 0 & 0 \\\\ 0 & 0 & 0 & 0.5 & 0 & 1 \\\\ 0 & 0 & 0 & 0 & 1 & 0.5 \\\\ 0 & 0 & 0 & 0 & 0 & 0.5\\end{bmatrix} = \\begin{bmatrix}1 & 0 & 0 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0 & 0 & 0 \\\\ 0 & 0 & 1 & 1 & 0 & 0 \\\\ 0 & 0 & 0 & 0 & 1 & 1 \\end{bmatrix} = A$$\n",
    "\n",
    "Thus, we can see that $E_i W = P$ and $E_o W = A$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "     \n",
    "#### (PTS:0-2)\n",
    "Consider two policies with the following actions chosen from each state\n",
    "\\begin{align*}\n",
    "\\textbf{Policy 1:} \\qquad \\quad & \\text{State 1: Action 1}, \\quad \\text{State 2: Action 2}, \\\\\n",
    "& \\text{State 3: Action 4}, \\quad \\text{State 4: Action 6} \\\\\n",
    "\\textbf{Policy 2:} \\qquad \\quad &\\text{State 1: Action 1}, \\quad \\text{State 2: Action 2}, \\\\\n",
    "& \\text{State 3: }\n",
    "\\begin{matrix}\n",
    "\\text{50\\% Action 3} \\\\\n",
    "\\text{50\\% Action 4}\n",
    "\\end{matrix}, \\quad\n",
    "\\text{State 4: }\n",
    "\\begin{matrix}\n",
    "\\text{50\\% Action 5} \\\\\n",
    "\\text{50\\% Action 6}\n",
    "\\end{matrix}\n",
    "\\end{align*}\n",
    "Write each policy in matrix form $\\Pi \\in \\mathbb{R}^{6 \\times 4}$.  Compute the corresponding Markov matrix $M = P\\Pi$.  Also show that $A\\Pi = I$ for each policy. "
   ]
  },
  {
   "source": [
    "The policy matrices $\\Pi$ can be written as follows\n",
    "\n",
    "$$\\Pi_1 = \\begin{bmatrix}1 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0 \\\\ 0 & 0 & 0 & 0 \\\\ 0 & 0 & 1 & 0 \\\\ 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 1 \\end{bmatrix}$$\n",
    "\n",
    "$$\\Pi_2 = \\begin{bmatrix}1 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0 \\\\ 0 & 0 & 0.5 & 0 \\\\ 0 & 0 & 0.5 & 0 \\\\ 0 & 0 & 0 & 0.5 \\\\ 0 & 0 & 0 & 0.5 \\end{bmatrix}$$\n",
    "\n",
    "The corresponding Markov matrices are computed as follows,\n",
    "\n",
    "$$M_1 = P\\Pi_1 = \\begin{bmatrix}0 & 1 & 0 & 0 & 1 & 0.5 \\\\ 0 & 0 & 0 & 0.5 & 0 & 0 \\\\ 1 & 0 & 0 & 0 & 0 & 0.5 \\\\ 0 & 0 & 1 & 0.5 & 0 & 0 \\end{bmatrix}*\\begin{bmatrix}1 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0 \\\\ 0 & 0 & 0 & 0 \\\\ 0 & 0 & 1 & 0 \\\\ 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 1 \\end{bmatrix}$$\n",
    "$$M_1 = \\begin{bmatrix}0 & 1 & 0 & 0.5 \\\\ 0 & 0 & 0.5 & 0 \\\\ 1 & 0 & 0 & 0.5 \\\\ 0 & 0 & 0.5 & 0 \\end{bmatrix}$$\n",
    "\n",
    "$$M_2 = P\\Pi_2 = \\begin{bmatrix}0 & 1 & 0 & 0 & 1 & 0.5 \\\\ 0 & 0 & 0 & 0.5 & 0 & 0 \\\\ 1 & 0 & 0 & 0 & 0 & 0.5 \\\\ 0 & 0 & 1 & 0.5 & 0 & 0 \\end{bmatrix}*\\begin{bmatrix}1 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0 \\\\ 0 & 0 & 0.5 & 0 \\\\ 0 & 0 & 0.5 & 0 \\\\ 0 & 0 & 0 & 0.5 \\\\ 0 & 0 & 0 & 0.5 \\end{bmatrix}$$\n",
    "$$M_2 = \\begin{bmatrix}0 & 1 & 0 & 0.75 \\\\ 0 & 0 & 0.25 & 0 \\\\ 1 & 0 & 0 & 0.25 \\\\ 0 & 0 & 0.75 & 0 \\end{bmatrix}$$\n",
    "\n",
    "Proving that $A\\Pi = I$ for each policy,\n",
    "\n",
    "$$A\\Pi_1 = \\begin{bmatrix}1 & 0 & 0 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0 & 0 & 0 \\\\ 0 & 0 & 1 & 1 & 0 & 0 \\\\ 0 & 0 & 0 & 0 & 1 & 1 \\end{bmatrix}*\\begin{bmatrix}1 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0 \\\\ 0 & 0 & 0 & 0 \\\\ 0 & 0 & 1 & 0 \\\\ 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 1 \\end{bmatrix} = \\begin{bmatrix}1 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0 \\\\ 0 & 0 & 1 & 0 \\\\ 0 & 0 & 0 & 1 \\end{bmatrix} = I$$\n",
    "\n",
    "$$A\\Pi_2 = \\begin{bmatrix}1 & 0 & 0 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0 & 0 & 0 \\\\ 0 & 0 & 1 & 1 & 0 & 0 \\\\ 0 & 0 & 0 & 0 & 1 & 1 \\end{bmatrix}*\\begin{bmatrix}1 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0 \\\\ 0 & 0 & 0.5 & 0 \\\\ 0 & 0 & 0.5 & 0 \\\\ 0 & 0 & 0 & 0.5 \\\\ 0 & 0 & 0 & 0.5 \\end{bmatrix} = \\begin{bmatrix}1 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0 \\\\ 0 & 0 & 1 & 0 \\\\ 0 & 0 & 0 & 1 \\end{bmatrix} = I$$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (PTS:0-4)\n",
    "The stationary (state) distribution associated with each Markov chain is the solution to the equation $\\rho = M \\rho$. Compute this stationary distribution by finding the eigenvector with eigenvalue 1. (You can use the function eig in Matlab or numpy.linalg.eig in Python.). Make sure to scale the eigenvector so that it is an appropriate probability distribution that sums to 1 and has all positive values.  Compute the corresponding action distribution $y$ as $y = \\Pi \\rho$.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[-0.5+0.7j -0.5-0.7j  1. +0.j   0. +0.j ] \n [[-0.2+0.6j -0.2-0.6j  0.5+0.j  -0.4+0.j ]\n [-0.2-0.3j -0.2+0.3j  0.3+0.j  -0.4+0.j ]\n [ 0.6+0.j   0.6-0.j   0.7+0.j   0. +0.j ]\n [-0.2-0.3j -0.2+0.3j  0.3+0.j   0.8+0.j ]]\n[-0.5+0.8j -0.5-0.8j  1. +0.j   0. +0.j ] \n [[ 0.6+0.j   0.6-0.j   0.5+0.j  -0.2+0.j ]\n [-0.1+0.1j -0.1-0.1j  0.2+0.j  -0.6+0.j ]\n [-0.2-0.6j -0.2+0.6j  0.7+0.j   0. +0.j ]\n [-0.3+0.4j -0.3-0.4j  0.5+0.j   0.8+0.j ]]\n"
     ]
    }
   ],
   "source": [
    "M_1 = np.matrix([[0, 1, 0, 0.5], [0, 0, 0.5, 0], [1, 0, 0, 0.5], [0, 0, 0.5, 0]])\n",
    "M_2 = np.matrix([[0, 1, 0, 0.75], [0, 0, 0.25, 0], [1, 0, 0, 0.25], [0, 0, 0.75, 0]])\n",
    "\n",
    "w, v = np.linalg.eig(M_1)\n",
    "print(np.round_(w, decimals=1), '\\n', np.round_(v, decimals=1))\n",
    "\n",
    "w, v = np.linalg.eig(M_2)\n",
    "print(np.round_(w, decimals=1), '\\n', np.round_(v, decimals=1))"
   ]
  },
  {
   "source": [
    "The stationary distribution as identified using numpy.linalg.eig (as coded above) results in,\n",
    "\n",
    "$$\\rho_1 = \\begin{bmatrix}0.32 & 0.32 & 0.36 & 0 \\end{bmatrix}^T$$\n",
    "$$\\rho_2 = \\begin{bmatrix}0.18 & 0.18 & 0.64 & 0 \\end{bmatrix}^T$$\n",
    "\n",
    "The corresponding action distribution $y$ is calculated as,\n",
    "\n",
    "$$y_1 = \\Pi_1 \\rho_1 = \\begin{bmatrix}1 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0 \\\\ 0 & 0 & 0 & 0 \\\\ 0 & 0 & 1 & 0 \\\\ 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 1 \\end{bmatrix} *\\begin{bmatrix}0.32 \\\\ 0.32 \\\\ 0.36 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix}0.32 \\\\ 0.32 \\\\ 0 \\\\ 0.36 \\\\ 0 \\\\ 0\\end{bmatrix}$$\n",
    "\n",
    "$$y_2 = \\Pi_2 \\rho_2 = \\begin{bmatrix}1 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0 \\\\ 0 & 0 & 0.5 & 0 \\\\ 0 & 0 & 0.5 & 0 \\\\ 0 & 0 & 0 & 0.5 \\\\ 0 & 0 & 0 & 0.5 \\end{bmatrix} *\\begin{bmatrix}0.18 \\\\ 0.18 \\\\ 0.64 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix}0.18 \\\\ 0.18 \\\\ 0.32 \\\\ 0.32 \\\\ 0 \\\\ 0\\end{bmatrix}$$\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (PTS:0-2)\n",
    "Show that each $y$ from the previous part satisfies $Py = Ay$ and $\\mathbf{1}^Ty = 1$. Compute the corresponding edge mass vector for each $x = Wy$.  Show that $x$ is in the nullspace of $E = E_i - E_o$.  "
   ]
  },
  {
   "source": [
    "For each $y$, we can show that $Py = Ay$ and $1^Ty = 1$,\n",
    "\n",
    "$$Py_1 = \\begin{bmatrix}0 & 1 & 0 & 0 & 1 & 0.5 \\\\ 0 & 0 & 0 & 0.5 & 0 & 0 \\\\ 1 & 0 & 0 & 0 & 0 & 0.5 \\\\ 0 & 0 & 1 & 0.5 & 0 & 0 \\end{bmatrix}*\\begin{bmatrix}0.32 \\\\ 0.32 \\\\ 0 \\\\ 0.36 \\\\ 0 \\\\ 0\\end{bmatrix} = \\begin{bmatrix}0.32 \\\\ 0.18 \\\\ 0.32 \\\\ 0.18 \\end{bmatrix}$$\n",
    "\n",
    "$$Ay_1 = \\begin{bmatrix}1 & 0 & 0 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0 & 0 & 0 \\\\ 0 & 0 & 1 & 1 & 0 & 0 \\\\ 0 & 0 & 0 & 0 & 1 & 1 \\end{bmatrix}*\\begin{bmatrix}0.32 \\\\ 0.32 \\\\ 0 \\\\ 0.36 \\\\ 0 \\\\ 0\\end{bmatrix} = \\begin{bmatrix}0.32 \\\\ 0.32 \\\\ 0.36 \\\\ 0 \\end{bmatrix}$$\n",
    "\n",
    "$$Py_2 = \\begin{bmatrix}0 & 1 & 0 & 0 & 1 & 0.5 \\\\ 0 & 0 & 0 & 0.5 & 0 & 0 \\\\ 1 & 0 & 0 & 0 & 0 & 0.5 \\\\ 0 & 0 & 1 & 0.5 & 0 & 0 \\end{bmatrix} * \\begin{bmatrix}0.32 \\\\ 0.32 \\\\ 0 \\\\ 0.36 \\\\ 0 \\\\ 0\\end{bmatrix} = \\begin{bmatrix}0.18 \\\\ 0.16 \\\\ 0.18 \\\\ 0.48 \\end{bmatrix}$$\n",
    "\n",
    "$$Ay_2 = \\begin{bmatrix}1 & 0 & 0 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0 & 0 & 0 \\\\ 0 & 0 & 1 & 1 & 0 & 0 \\\\ 0 & 0 & 0 & 0 & 1 & 1 \\end{bmatrix}*\\begin{bmatrix}0.18 \\\\ 0.18 \\\\ 0.32 \\\\ 0.32 \\\\ 0 \\\\ 0\\end{bmatrix} = \\begin{bmatrix}0.18 \\\\ 0.18 \\\\ 0.64 \\\\ 0 \\end{bmatrix}$$\n",
    "\n",
    "\n",
    "$$1^Ty_1 = \\begin{bmatrix}1 & 1 & 1 & 1 & 1 & 1\\end{bmatrix}*\\begin{bmatrix}0.32 \\\\ 0.32 \\\\ 0 \\\\ 0.36 \\\\ 0 \\\\ 0\\end{bmatrix} = 1$$\n",
    "\n",
    "$$1^Ty_2 = \\begin{bmatrix}1 & 1 & 1 & 1 & 1 & 1\\end{bmatrix}*\\begin{bmatrix}0.18 \\\\ 0.18 \\\\ 0.32 \\\\ 0.32 \\\\ 0 \\\\ 0\\end{bmatrix} = 1$$\n",
    "\n",
    "The corresponding edge mass vector is calculated as,\n",
    "\n",
    "$$x_1 = Wy_1 = \\begin{bmatrix}1 & 0 & 0 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0 & 0 & 0 \\\\ 0 & 0 & 1 & 0.5 & 0 & 0 \\\\ 0 & 0 & 0 & 0.5 & 0 & 1 \\\\ 0 & 0 & 0 & 0 & 1 & 0.5 \\\\ 0 & 0 & 0 & 0 & 0 & 0.5\\end{bmatrix}*\\begin{bmatrix}0.32 \\\\ 0.32 \\\\ 0 \\\\ 0.36 \\\\ 0 \\\\ 0\\end{bmatrix} = \\begin{bmatrix}0.32 \\\\ 0.32 \\\\ 0 \\\\ 0.18 \\\\ 0 \\\\ 0\\end{bmatrix}$$\n",
    "\n",
    "$$x_2 = Wy_2 = \\begin{bmatrix}1 & 0 & 0 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0 & 0 & 0 \\\\ 0 & 0 & 1 & 0.5 & 0 & 0 \\\\ 0 & 0 & 0 & 0.5 & 0 & 1 \\\\ 0 & 0 & 0 & 0 & 1 & 0.5 \\\\ 0 & 0 & 0 & 0 & 0 & 0.5\\end{bmatrix}*\\begin{bmatrix}0.18 \\\\ 0.18 \\\\ 0.48 \\\\ 0.16 \\\\ 0 \\\\ 0\\end{bmatrix}$$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Infinite Horizon, Average Reward\n",
    "    \n",
    "Consider the following optimization problem for finding the optimal steady-state action  distribution $y  \\in \\mathbb{R}^{|\\mathcal{A}|}$\n",
    "\\begin{align}\n",
    "\\max_{y} & \\quad r^Ty \\\\\n",
    "\\text{s.t.} & \\quad Py = Ay, \\ \\mathbf{1}^Ty = 1, \\ y \\geq 0 \\notag\n",
    "\\end{align}\n",
    "for reward vector $r \\in \\mathbb{R}^{|\\mathcal{A}|}$.  \n",
    "    \n",
    "#### (PTS:0-2)\n",
    "Write the dual optimization problem with dual variables $\\lambda \\in \\mathbb{R}$ associated with the constraint $\\mathbf{1}^Ty=1$, $v \\in \\mathbb{R}^{|\\mathcal{S}|}$ associated with constraint $Py=Ay$, $\\mu \\in \\mathbb{R}^{|\\mathcal{A}|}_+$ associated with the constraint $y \\geq 0$. "
   ]
  },
  {
   "source": [
    "We can write the dual optimization problem as,\n",
    "\n",
    "$$\\max_limits_{v, \\lambda, \\mu} \\lambda$$\n",
    "$$\\text{s.t.} \\; \\lambda 1^T + v^TA = r^T + v^TP + \\mu^T, \\mu \\geq 0$$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        \n",
    "#### (PTS:0-2)\n",
    "The KKT conditions at optimum (for either the primal or dual problem) are given by\n",
    "\\begin{align*}\n",
    "r^T - \\lambda \\mathbf{1}^T + v^T(P-A) + \\mu^T & = 0, \\quad \\mu \\geq 0 \\\\\n",
    "Py-Ay = 0, \\quad \\mathbf{1}^Ty & = 1, \\quad y \\geq 0  \\\\\n",
    "\\mu^Ty & = 0 \n",
    "\\end{align*}\n",
    "        \n",
    "Use these conditions to show that $\\lambda$ is an upper bound on the primal objective $r^Ty$ for any feasible $y$.  What does $\\mu^Ty$ represent for a specific $y$?  What does the condition $\\mu^Ty = 0$ imply about the optimal $y$?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        \n",
    "#### (PTS:0-4)\n",
    "Use cvx or cvxpy to solve the above optimization problem for the transition kernel given initially and each reward vector\n",
    "\\begin{align*}\n",
    "r^T & =\n",
    "\\begin{bmatrix}\n",
    "1 & 2 & 3 & 4 & 5 & 6\n",
    "\\end{bmatrix} \\\\\n",
    "r^T & =\n",
    "\\begin{bmatrix}\n",
    "1 & 1 & 1 & 1 & 1 & 1\n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "What is the optimal joint distribution $y$ in each case?  What is the expected average reward $r^Ty$ in each case? "
   ]
  },
  {
   "source": [
    "The solutions to the above optimization problem are given as outputs from the code below. In addition, each expected average reward $r^Ty$ is printed out as well."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "For r = [[1 2 3 4 5 6]]\nThe optimal value is 3.8\n\nA solution y is:\n[[0.2]\n [0. ]\n [0.4]\n [0. ]\n [0. ]\n [0.4]]\n\nA dual solution is: \n[[ 1.4]\n [ 2.1]\n [-1.4]\n [-2.2]] \nand \n [[3.8]]\nExpected Average Reward: [[3.8]]\n"
     ]
    }
   ],
   "source": [
    "# Establish our values for input parameters\n",
    "P = np.matrix([[0, 1, 0, 0, 1, 0.5], [0, 0, 0, 0.5, 0, 0], [1, 0, 0, 0, 0, 0.5], [0, 0, 1, 0.5, 0, 0]])\n",
    "A = np.matrix([[1, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0], [0, 0, 1, 1, 0, 0], [0, 0, 0, 0, 1, 1]])\n",
    "r_T = np.matrix([1, 2, 3, 4, 5, 6]) # Define r_T as reward vector\n",
    "one_T = np.matrix([1, 1, 1, 1, 1, 1])\n",
    "\n",
    "# Define and solve the CVXPY problem.\n",
    "y = cp.Variable(shape=(6, 1))\n",
    "prob = cp.Problem(cp.Maximize(r_T @ y), [P @ y == A @ y, one_T @ y == 1, y >= 0])\n",
    "prob.solve()\n",
    "\n",
    "# Print result.\n",
    "print('For r =', r_T)\n",
    "print(\"The optimal value is\", np.round_(prob.value, decimals = 1))\n",
    "print(\"\\nA solution y is:\")\n",
    "print(np.round_(y.value, decimals=1))\n",
    "print(\"\\nA dual solution is: \")\n",
    "print(np.round_(prob.constraints[0].dual_value, decimals=1), '\\nand \\n', np.round_(prob.constraints[1].dual_value, decimals=1))\n",
    "\n",
    "y1 = y.value\n",
    "\n",
    "print('Expected Average Reward:', r_T @ y.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "For r = [[1 1 1 1 1 1]]\nThe optimal value is 1.0\n\nA solution y is:\n[[0.3]\n [0.1]\n [0.2]\n [0.2]\n [0.1]\n [0.2]]\n\nA dual solution is: \n[[-0.]\n [-0.]\n [-0.]\n [-0.]] \nand \n [[1.]]\nExpected Average Reward: [[1.]]\n"
     ]
    }
   ],
   "source": [
    "r_T = np.matrix([1, 1, 1, 1, 1, 1]) # Define r_T as reward vector\n",
    "\n",
    "# Define and solve the CVXPY problem.\n",
    "y = cp.Variable(shape=(6, 1))\n",
    "prob = cp.Problem(cp.Maximize(r_T @ y), [P @ y == A @ y, one_T @ y == 1, y >= 0])\n",
    "prob.solve()\n",
    "\n",
    "# Print result.\n",
    "print('For r =', r_T)\n",
    "print(\"The optimal value is\", np.round_(prob.value, decimals = 1))\n",
    "print(\"\\nA solution y is:\")\n",
    "print(np.round_(y.value, decimals=1))\n",
    "print(\"\\nA dual solution is: \")\n",
    "print(np.round_(prob.constraints[0].dual_value, decimals=1), '\\nand \\n', np.round_(prob.constraints[1].dual_value, decimals=1))\n",
    "\n",
    "y2 = y.value\n",
    "\n",
    "print('Expected Average Reward:', r_T @ y.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (PTS:0-2)\n",
    "What is the steady-state state distribution associated with each solution $\\rho=Ay$?  \n",
    "What is the optimal policy associated with $y$?  Use the formula\n",
    "\\begin{align*}\n",
    "    (\\pi_s)_a = \\frac{y_a}{\\rho_s} = \\frac{y_a}{\\sum_{a \\in \\mathcal{A}_s} y_a}\n",
    "\\end{align*}\n",
    "You could also put the policy in matrix form using the formula\n",
    "\\begin{align*}\n",
    "\\Pi = \\text{diag}(y)A^T\\text{diag}(\\rho)^{-1}\n",
    "\\end{align*}\n",
    "        "
   ]
  },
  {
   "source": [
    "The steady-state distributions and policies are outputted to console using the code below."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Rho 1:\n [[0.2]\n [0. ]\n [0.4]\n [0.4]]\nRho 2:\n [[0.3]\n [0.1]\n [0.4]\n [0.3]]\nPi 1:\n [[1. 0. 0. 0.]\n [0. 1. 0. 0.]\n [0. 0. 1. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 1.]]\nPi 2:\n [[1.   0.   0.   0.  ]\n [0.   1.   0.   0.  ]\n [0.   0.   0.49 0.  ]\n [0.   0.   0.51 0.  ]\n [0.   0.   0.   0.42]\n [0.   0.   0.   0.58]]\n"
     ]
    }
   ],
   "source": [
    "A = np.matrix([[1, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0], [0, 0, 1, 1, 0, 0], [0, 0, 0, 0, 1, 1]])\n",
    "\n",
    "rho_1 = A @ y1\n",
    "rho_2 = A @ y2\n",
    "\n",
    "print('Rho 1:\\n', np.round_(rho_1, decimals=1))\n",
    "print('Rho 2:\\n', np.round_(rho_2, decimals=1))\n",
    "\n",
    "Pi_1 = np.diag(np.squeeze(y1)) @ A.T @ np.linalg.inv(np.diag(np.squeeze(np.asarray(rho_1))))\n",
    "print('Pi 1:\\n', np.round_(Pi_1, decimals=1))\n",
    "\n",
    "Pi_2 = np.diag(np.squeeze(y2)) @ A.T @ np.linalg.inv(np.diag(np.squeeze(np.asarray(rho_2))))\n",
    "print('Pi 2:\\n', np.round_(Pi_2, decimals=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   \n",
    "#### (PTS:0-2) Now suppose you apply the policy \n",
    "\\begin{align*}\n",
    "\\Pi = \\begin{bmatrix}\n",
    "1 & 0 & 0 & 0 \\\\\n",
    "0 & 1 & 0 & 0 \\\\\n",
    "0 & 0 & 0.2 & 0 \\\\\n",
    "0 & 0 & 0.8 & 0 \\\\\n",
    "0 & 0 & 0 & 0.2 \\\\\n",
    "0 & 0 & 0 & 0.8 \n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "What reward do you achieve in each case? (Hint: compute $\\rho$ such that $\\rho = P\\Pi \\rho$ and then $y$ using $y=\\Pi \\rho$.)\n",
    "How much does this reward differ from the optimal average reward?  How does this difference relate to the quantity $\\mu^Ty$ where $\\mu$ is the optimal dual variable?\n",
    "\n",
    "    "
   ]
  },
  {
   "source": [
    "The reward and average reward are calculated below using the code. We can see that our rewards are smaller when compared to our previously calculated values."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Reward:\n [[-0.6]\n [-0.6]\n [ 0.1]\n [ 0.6]\n [-0. ]\n [-0. ]]\nNew Average Reward (Case 1):\n [[0.79264503]]\nNew Average Reward (Case 2):\n [[-0.53027565]]\n"
     ]
    }
   ],
   "source": [
    "Pi_new = np.matrix([[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 0.2, 0], [0, 0, 0.8, 0], [0, 0, 0, 0.2], [0, 0, 0, 0.8]])\n",
    "\n",
    "w, v = np.linalg.eig(P @ Pi_new)\n",
    "#print(np.round_(w, decimals=1), '\\n', np.round_(v, decimals=1))\n",
    "\n",
    "rho_new = np.real(v[2, :])\n",
    "y_new = Pi_new @ rho_new.T\n",
    "print('Reward:\\n', np.round_(y_new, decimals=1))\n",
    "\n",
    "r_T = np.matrix([1, 2, 3, 4, 5, 6]) # Define r_T as reward vector\n",
    "print('New Average Reward (Case 1):\\n', r_T @ y_new)\n",
    "r_T = np.matrix([1, 1, 1, 1, 1, 1]) # Define r_T as reward vector\n",
    "print('New Average Reward (Case 2):\\n', r_T @ y_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    \n",
    "\n",
    "\n",
    "## 3. Finite Horizon, Total Reward\n",
    "    \n",
    "Consider the following optimization problem for finding the optimal finite horizon policy.  \n",
    "\\begin{align}\n",
    "\\max_{y(t), \\ t \\in \\mathcal{T}} & \\quad \\sum_{t=0}^{T-1} r(t)^Ty(t) + g^TAy(T) \\\\\n",
    "\\text{s.t.} & \\quad Ay(0) = \\rho(0), \\quad y(0) \\geq 0 \\notag \\\\\n",
    "& \\quad Ay(t+1) = Py(t), \\quad y(t+1) \\geq 0, \\ \\ t \\in \\mathcal{T} \\notag\n",
    "\\end{align}\n",
    "where $\\mathcal{T} = \\{0,\\dots, T-1\\}$, $\\rho(0) \\in \\mathbb{R}^{|\\mathcal{S}|}$ is a given initial state distribution, and $g \\in \\mathbb{R}^{|\\mathcal{S}|}$ is a final cost on each of the states.  \n",
    "    \n",
    "#### (PTS:0-4)\n",
    "Write the dual optimization problem with dual variables $v(0) \\in \\mathbb{R}^{|\\mathcal{S}|}$ associated with the constraint $Ay(0)=\\rho(0)$, $v(t+1) \\in \\mathbb{R}^{|\\mathcal{S}|}$ associated with constraint $Py(t)=Ay(t+1)$, and $\\mu(t) \\in \\mathbb{R}^{|\\mathcal{A}|}_+$ associated with the constraint $y(t) \\geq 0$. \n",
    "        \n",
    "        "
   ]
  },
  {
   "source": [
    "We can write the dual optimization problem such that,\n",
    "\n",
    "$$\\min\\limits_{v, \\mu} v(0)^T \\rho(0)$$\n",
    "$$\\text{s.t.} \\; v(T)^TA = g^TA + \\mu(T)^T, \\mu(T) \\geq 0$$\n",
    "$$v(t)^TA = r(t)^T + v(t+1)^TP + \\mu(t)^T, \\mu(t) \\geq 0$$\n",
    "\n",
    "where $t = \\{0, ..., T-1\\}$."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### (PTS:0-4)\n",
    "The KKT optimality conditions for the primal and dual optimization problems\n",
    "are given by\n",
    "\\begin{align*}\n",
    "g^TA - v(T)A + \\mu(T)^T & = 0, \\quad \\mu(T) \\geq 0  \\\\\n",
    "r(t)^T +v(t+1)^TP-v(t)^TA+\\mu(t)^T & = 0, \\quad \\mu(t) \\geq 0, \\quad t \\in \\mathcal{T} \\\\\n",
    "Ay(0) & = \\rho(0), \\quad y(0) \\geq 0  \\\\\n",
    "Ay(t+1) & = Py(t), \\quad y(t+1) \\geq 0, \\quad t \\in \\mathcal{T} \\\\\n",
    "\\mu(t)^Ty(t) & = 0, \\quad t \\in \\mathcal{T},\\ t=T\n",
    "\\end{align*}\n",
    "                \n",
    "Use these conditions to show that $v(0)^T\\rho(0)$ is an upper bound on the primal objective $\\sum_t r(t)^Ty(t) + g^TAy(T)$ for any feasible $y(t)$ that satisfies the mass flow equations.  What does $\\sum_t \\mu(t)^Ty(t)$ represent for a specific mass flow $y(t), t \\in \\mathcal{T}$."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (PTS:0-4)\n",
    "Use cvx or cvxpy to solve the above optimization problem for the MDP given initially with the following rewards\n",
    "\\begin{align*}\n",
    "r(t)^T & =\n",
    "\\begin{bmatrix}\n",
    "2 & 1 & 2 & 1 & 2 & 1\n",
    "\\end{bmatrix} \\text{ for } t \\in \\mathcal{T}, \\quad g^T =\n",
    "\\begin{bmatrix}\n",
    "1 & 1 & 1 & 1\n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "for ten time steps\n",
    " $T=10$\n",
    "and initial distribution $\\rho(0) = \\begin{bmatrix}0.25 & 0.25 & 0.25 & 0.25 \\end{bmatrix}^T$\n",
    "\n",
    "        \n",
    "What is the optimal action distribution $y(t)$ at each time step?  What is the expected total reward $\\sum_t r(t)^Ty(t)$? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        \n",
    "#### (PTS:0-4)\n",
    "What is the policy $\\Pi(t)$ chosen at each time step?  Use the formula\n",
    "\\begin{align*}\n",
    "    (\\pi_s)_a(t) = \\tfrac{y_a(t)}{\\rho_s(t)} = \\tfrac{y_a(t}{\\sum_{a \\in \\mathcal{A}_s} y_a(t)}\n",
    "\\end{align*}\n",
    "where $\\rho(t) = Ay(t)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (PTS:0-4) Now suppose you apply the policy \n",
    "\\begin{align*}\n",
    "\\Pi(t) = \\begin{bmatrix}\n",
    "1 & 0 & 0 & 0 \\\\\n",
    "0 & 1 & 0 & 0 \\\\\n",
    "0 & 0 & 0.2 & 0 \\\\\n",
    "0 & 0 & 0.8 & 0 \\\\\n",
    "0 & 0 & 0 & 0.2 \\\\\n",
    "0 & 0 & 0 & 0.8 \n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "at each time step.\n",
    "Start by computing $y(0) = \\Pi(0)\\rho(0)$. $\\rho(t)$ is then given by $Py(0) = \\rho(1)$. Use $\\rho(1)$ to compute $y(1)= \\Pi(1)\\rho(1)$, etc.  What total reward do you achieve? What is the quantity $\\sum_t \\mu(t)^Ty(t)$?  How does this relate the total reward to the optimal total reward?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}